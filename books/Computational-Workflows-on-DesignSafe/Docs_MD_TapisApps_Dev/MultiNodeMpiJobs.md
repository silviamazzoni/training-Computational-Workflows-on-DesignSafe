## Running Multi-Node & MPI Jobs in Tapis
Tapis provides a convenient way to run MPI programs. In the Tapis application definition, there is a boolean flag isMpi which indicates whether the app should be launched as an MPI job[13]. By default this is false (meaning the job will be treated as a single-process or non-MPI job). If you set isMpi=true for the application (or override it in the job submission), the Tapis job service will automatically insert the MPI launcher command when running your executable[14]. In practice, this means tapisjob.sh will prepend something like mpirun (or the appropriate launcher command for that system) to your tapisjob_app.sh call. The specific command it inserts is determined by the mpiCmd setting for the system or app – for example, it could use the generic mpirun, or a system-specific launcher like TACC’s ibrun, depending on configuration[15]. The mpiCmd is typically defined in the execution system’s profile or the app; it tells Tapis what MPI launch command to use on that cluster (including any flags needed). If isMpi=true and mpiCmd is set, Tapis will construct the command line so that your application runs across all the allocated nodes/processors using MPI[14].
Avoiding Double MPI Launch: It’s important to note that you should either let Tapis handle the MPI launch or do it yourself, but not both. If your tapisjob_app.sh (or executable) already calls mpirun or spawns MPI processes internally, you should leave isMpi=false (the default). Otherwise, you’d end up with two layers of MPI launch which can conflict. The Tapis docs highlight that if the application itself manages MPI, you should disable the Tapis MPI flag to prevent “conflicting MPI calls”[14]. In summary, for an MPI job you have two options:

- **Tapis-managed MPI:** Set isMpi=true and do not call mpirun in your script. Simply invoke the parallel program normally, and Tapis will do mpirun <your_program> for you (with the correct number of processes based on nodeCount and coresPerNode). This is convenient if you want Tapis to handle the details.
- **User-managed MPI:** Leave isMpi=false and call the MPI launcher yourself in tapisjob_app.sh (e.g., srun -n 64 ./myapp or mpirun -n 64 ./myapp, depending on the system). This gives you manual control similar to a normal Slurm script. In this case, Tapis will treat it like a regular job and just execute your script (which itself will start the MPI run). Both approaches ultimately achieve the same result – running across the allocated nodes – so use whichever fits your needs and the way your application is written.

**Utilizing Cores vs. Nodes:**
Tapis allows requesting a certain number of cores per node (coresPerNode). If you use the Tapis MPI launch (isMpi=true), typically it will launch a number of MPI ranks equal to nodeCount * coresPerNode (unless overridden). For example, if you request 2 nodes with 16 cores each, Tapis can launch 32 MPI processes total across the nodes. If you are launching manually, you can use Slurm environment variables like SLURM_NTASKS or SLURM_CPUS_ON_NODE inside your script. Tapis will have already set up the Slurm job with those counts. There is also a Tapis-specific env var _tapisNodes (and _tapisCoresPerNode) available, which simply reflect what was requested[16]. In essence, within tapisjob_app.sh you have the full Slurm context (including SLURM_NODELIST, etc.), just as in a normal batch job. This means you can use any parallel or distributed computing approach that you would in a manual script.
